{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd645fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC 10-Q/10-K -> 3-statement extractor (Balance Sheet, Income Statement, Cash Flow)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Filing type must be '10-K' or '10-Q'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 486\u001b[39m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# Run interactively (works in notebook too)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[43minteractive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 430\u001b[39m, in \u001b[36minteractive\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    428\u001b[39m form = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFiling type (10-K or 10-Q): \u001b[39m\u001b[33m\"\u001b[39m).strip().upper()\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m form \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33m10-K\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m10-Q\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFiling type must be \u001b[39m\u001b[33m'\u001b[39m\u001b[33m10-K\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m10-Q\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    432\u001b[39m mode = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMode (latest/all): \u001b[39m\u001b[33m\"\u001b[39m).strip().lower()\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mlatest\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m}:\n",
      "\u001b[31mValueError\u001b[39m: Filing type must be '10-K' or '10-Q'."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (Required)\n",
    "# ============================================================\n",
    "USER_AGENT = \"your.email@gmail.com\"  # <-- set this\n",
    "MIN_SECONDS_BETWEEN_REQUESTS = 0.22  # conservative (SEC guideline is <= 10 req/sec)\n",
    "BASE_OUTPUT_DIR = Path(r\"C:\\Users\\reigh\\Desktop\\Fin Statements\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATA STRUCTURES\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class FilingRef:\n",
    "    cik: int\n",
    "    company: str\n",
    "    ticker: str\n",
    "    form: str\n",
    "    filing_date: str\n",
    "    accession_number: str\n",
    "    primary_document: Optional[str]\n",
    "    report_date: Optional[str]\n",
    "\n",
    "    @property\n",
    "    def cik_padded(self) -> str:\n",
    "        return str(self.cik).zfill(10)\n",
    "\n",
    "    @property\n",
    "    def accession_no_dashes(self) -> str:\n",
    "        return self.accession_number.replace(\"-\", \"\")\n",
    "\n",
    "    @property\n",
    "    def archive_dir(self) -> str:\n",
    "        return f\"https://www.sec.gov/Archives/edgar/data/{self.cik}/{self.accession_no_dashes}/\"\n",
    "\n",
    "    @property\n",
    "    def index_json_url(self) -> str:\n",
    "        return self.archive_dir + \"index.json\"\n",
    "\n",
    "    @property\n",
    "    def full_submission_txt_url(self) -> str:\n",
    "        return self.archive_dir + f\"{self.accession_number}.txt\"\n",
    "\n",
    "    def primary_doc_url(self) -> Optional[str]:\n",
    "        if not self.primary_document:\n",
    "            return None\n",
    "        return self.archive_dir + self.primary_document\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SEC CLIENT (rate-limited + retries)\n",
    "# ============================================================\n",
    "class SECClient:\n",
    "    def __init__(self, user_agent: str, min_interval_s: float = 0.22):\n",
    "        if not user_agent or \"@\" not in user_agent:\n",
    "            raise ValueError(\"Set USER_AGENT to something like 'Name (email@domain.com) - purpose'.\")\n",
    "        self.s = requests.Session()\n",
    "        self.s.headers.update({\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        })\n",
    "        self.min_interval_s = min_interval_s\n",
    "        self._last_request_ts = 0.0\n",
    "        self._ticker_map_cache = None\n",
    "\n",
    "    def _sleep_if_needed(self):\n",
    "        now = time.time()\n",
    "        dt = now - self._last_request_ts\n",
    "        if dt < self.min_interval_s:\n",
    "            time.sleep(self.min_interval_s - dt)\n",
    "\n",
    "    def get_json(self, url: str, max_retries: int = 6) -> dict:\n",
    "        for attempt in range(max_retries):\n",
    "            self._sleep_if_needed()\n",
    "            r = self.s.get(url, timeout=30)\n",
    "            self._last_request_ts = time.time()\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(min(2 ** attempt, 16))\n",
    "                continue\n",
    "\n",
    "            r.raise_for_status()\n",
    "\n",
    "        raise RuntimeError(f\"Failed GET JSON: {url}\")\n",
    "\n",
    "    def get_text(self, url: str, max_retries: int = 6) -> str:\n",
    "        for attempt in range(max_retries):\n",
    "            self._sleep_if_needed()\n",
    "            r = self.s.get(url, timeout=60)\n",
    "            self._last_request_ts = time.time()\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                return r.text\n",
    "\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(min(2 ** attempt, 16))\n",
    "                continue\n",
    "\n",
    "            r.raise_for_status()\n",
    "\n",
    "        raise RuntimeError(f\"Failed GET TEXT: {url}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Ticker -> CIK\n",
    "    # -------------------------\n",
    "    def load_ticker_map(self) -> Dict[str, Dict]:\n",
    "        if self._ticker_map_cache is not None:\n",
    "            return self._ticker_map_cache\n",
    "\n",
    "        url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "        raw = self.get_json(url)\n",
    "\n",
    "        m = {}\n",
    "        for _, row in raw.items():\n",
    "            t = row.get(\"ticker\", \"\").upper()\n",
    "            if t:\n",
    "                m[t] = {\"cik\": int(row[\"cik_str\"]), \"title\": row.get(\"title\", \"\")}\n",
    "\n",
    "        self._ticker_map_cache = m\n",
    "        return m\n",
    "\n",
    "    def cik_from_ticker(self, ticker: str) -> Tuple[int, str]:\n",
    "        tm = self.load_ticker_map()\n",
    "        key = ticker.upper().strip()\n",
    "        if key not in tm:\n",
    "            raise ValueError(f\"Ticker not found in SEC ticker map: {ticker}\")\n",
    "        return tm[key][\"cik\"], tm[key][\"title\"]\n",
    "\n",
    "    # -------------------------\n",
    "    # Submissions\n",
    "    # -------------------------\n",
    "    def get_submissions(self, cik: int) -> dict:\n",
    "        cik_padded = str(cik).zfill(10)\n",
    "        url = f\"https://data.sec.gov/submissions/CIK{cik_padded}.json\"\n",
    "        return self.get_json(url)\n",
    "\n",
    "    def get_submissions_file(self, filename: str) -> dict:\n",
    "        # filenames in submissions['filings']['files'] are relative to:\n",
    "        # https://data.sec.gov/submissions/<filename>\n",
    "        url = f\"https://data.sec.gov/submissions/{filename}\"\n",
    "        return self.get_json(url)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FILINGS: build full list (recent + older files)\n",
    "# ============================================================\n",
    "def collect_filings_for_form(\n",
    "    sec: SECClient,\n",
    "    ticker: str,\n",
    "    form_base: str,\n",
    "    mode: str = \"latest\",               # \"latest\" or \"all\"\n",
    "    include_amendments: bool = False,   # include /A\n",
    "    max_filings: int = 10\n",
    ") -> Tuple[str, int, str, List[FilingRef]]:\n",
    "    cik, title = sec.cik_from_ticker(ticker)\n",
    "    sub = sec.get_submissions(cik)\n",
    "\n",
    "    wanted_forms = {form_base}\n",
    "    if include_amendments:\n",
    "        wanted_forms.add(form_base + \"/A\")\n",
    "\n",
    "    # Start with \"recent\"\n",
    "    filings = []\n",
    "    def ingest_recent(recent_json: dict):\n",
    "        recent = recent_json.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms = recent.get(\"form\", [])\n",
    "        acc = recent.get(\"accessionNumber\", [])\n",
    "        filed = recent.get(\"filingDate\", [])\n",
    "        report = recent.get(\"reportDate\", [])\n",
    "        prim = recent.get(\"primaryDocument\", [])\n",
    "\n",
    "        for i, f in enumerate(forms):\n",
    "            if f in wanted_forms:\n",
    "                filings.append(FilingRef(\n",
    "                    cik=cik,\n",
    "                    company=title or sub.get(\"name\", \"\"),\n",
    "                    ticker=ticker.upper(),\n",
    "                    form=f,\n",
    "                    filing_date=filed[i],\n",
    "                    accession_number=acc[i],\n",
    "                    primary_document=prim[i] if i < len(prim) else None,\n",
    "                    report_date=report[i] if i < len(report) else None\n",
    "                ))\n",
    "\n",
    "    ingest_recent(sub)\n",
    "\n",
    "    # If mode is \"all\", also ingest older filings referenced in \"filings.files\"\n",
    "    if mode.lower() == \"all\":\n",
    "        for fobj in sub.get(\"filings\", {}).get(\"files\", []):\n",
    "            fname = fobj.get(\"name\")\n",
    "            if not fname:\n",
    "                continue\n",
    "            older = sec.get_submissions_file(fname)\n",
    "            ingest_recent(older)\n",
    "\n",
    "    # De-duplicate by accession number + form\n",
    "    uniq = {}\n",
    "    for fr in filings:\n",
    "        key = (fr.form, fr.accession_number)\n",
    "        uniq[key] = fr\n",
    "    filings = list(uniq.values())\n",
    "\n",
    "    # Sort by filing date desc\n",
    "    filings.sort(key=lambda x: x.filing_date, reverse=True)\n",
    "\n",
    "    if mode.lower() == \"latest\":\n",
    "        filings = filings[:1]\n",
    "    else:\n",
    "        filings = filings[:max_filings]\n",
    "\n",
    "    return ticker.upper(), cik, title, filings\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STATEMENT EXTRACTION (FilingSummary.xml route)\n",
    "# ============================================================\n",
    "def list_filing_files(sec: SECClient, filing: FilingRef) -> List[str]:\n",
    "    idx = sec.get_json(filing.index_json_url)\n",
    "    return [it[\"name\"] for it in idx.get(\"directory\", {}).get(\"item\", []) if \"name\" in it]\n",
    "\n",
    "def parse_filing_summary_xml(xml_text: str) -> List[Dict]:\n",
    "    soup = BeautifulSoup(xml_text, \"xml\")\n",
    "    reports = []\n",
    "    for r in soup.find_all(\"Report\"):\n",
    "        short = (r.ShortName.text or \"\").strip() if r.ShortName else \"\"\n",
    "        longn = (r.LongName.text or \"\").strip() if r.LongName else \"\"\n",
    "        html = (r.HtmlFileName.text or \"\").strip() if r.HtmlFileName else \"\"\n",
    "        if html:\n",
    "            reports.append({\"short\": short, \"long\": longn, \"html\": html})\n",
    "    return reports\n",
    "\n",
    "def score_report(rep: Dict, kind: str) -> int:\n",
    "    txt = (rep[\"short\"] + \" \" + rep[\"long\"]).lower()\n",
    "    s = 0\n",
    "\n",
    "    # Penalize obvious non-statements\n",
    "    if any(k in txt for k in [\"notes\", \"note\", \"exhibit\", \"schedule\", \"quarterly data\", \"controls and procedures\"]):\n",
    "        s -= 5\n",
    "\n",
    "    if kind == \"BS\":\n",
    "        if \"balance sheet\" in txt or \"financial position\" in txt:\n",
    "            s += 10\n",
    "        if \"consolidated\" in txt:\n",
    "            s += 2\n",
    "\n",
    "    if kind == \"IS\":\n",
    "        if any(k in txt for k in [\"income statement\", \"statement of operations\", \"operations\", \"earnings\"]):\n",
    "            s += 10\n",
    "        if \"comprehensive income\" in txt:\n",
    "            s += 1  # not always the main IS\n",
    "\n",
    "    if kind == \"CFS\":\n",
    "        if \"cash flow\" in txt or \"cash flows\" in txt:\n",
    "            s += 10\n",
    "        if \"supplemental\" in txt:\n",
    "            s -= 2\n",
    "\n",
    "    return s\n",
    "\n",
    "def pick_best_report(reports: List[Dict], kind: str) -> Optional[Dict]:\n",
    "    scored = sorted([(score_report(r, kind), r) for r in reports], key=lambda x: x[0], reverse=True)\n",
    "    if not scored:\n",
    "        return None\n",
    "    best_score, best = scored[0]\n",
    "    return best if best_score > 0 else None\n",
    "\n",
    "def parse_number(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        return x\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"-\", \"–\", \"—\", \"−\"}:\n",
    "        return None\n",
    "\n",
    "    # Remove common footnote markers\n",
    "    s = re.sub(r\"\\[\\d+\\]\", \"\", s)\n",
    "    s = s.replace(\",\", \"\").replace(\"$\", \"\").strip()\n",
    "\n",
    "    neg = False\n",
    "    if s.startswith(\"(\") and s.endswith(\")\"):\n",
    "        neg = True\n",
    "        s = s[1:-1].strip()\n",
    "\n",
    "    try:\n",
    "        v = float(s)\n",
    "        return -v if neg else v\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_statement_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out.columns = [str(c).strip() for c in out.columns]\n",
    "\n",
    "    # convert all except first col to numeric where possible\n",
    "    if out.shape[1] >= 2:\n",
    "        for c in out.columns[1:]:\n",
    "            out[c] = out[c].map(parse_number)\n",
    "\n",
    "    return out\n",
    "\n",
    "def table_score(kind: str, df: pd.DataFrame) -> int:\n",
    "    # heuristic: score tables that contain typical statement anchors in first column\n",
    "    if df.shape[1] < 2 or df.shape[0] < 6:\n",
    "        return -999\n",
    "\n",
    "    first_col = df.iloc[:, 0].astype(str).str.lower()\n",
    "    txt = \" \".join(first_col.tolist())\n",
    "\n",
    "    score = 0\n",
    "    if kind == \"BS\":\n",
    "        for k in [\"total assets\", \"total liabilities\", \"stockholders\", \"shareholders\", \"equity\"]:\n",
    "            if k in txt:\n",
    "                score += 3\n",
    "    elif kind == \"IS\":\n",
    "        for k in [\"net income\", \"revenue\", \"gross profit\", \"operating income\", \"earnings per share\"]:\n",
    "            if k in txt:\n",
    "                score += 3\n",
    "    elif kind == \"CFS\":\n",
    "        for k in [\"operating activities\", \"investing activities\", \"financing activities\", \"net cash\"]:\n",
    "            if k in txt:\n",
    "                score += 3\n",
    "\n",
    "    # Prefer larger/denser statement tables\n",
    "    score += min(df.shape[0], 60) // 10\n",
    "    score += min(df.shape[1], 10)\n",
    "\n",
    "    return score\n",
    "\n",
    "def extract_statement_tables(sec: SECClient, filing: FilingRef, report_html_filename: str, kind: str) -> Dict:\n",
    "    url = filing.archive_dir + report_html_filename\n",
    "    html = sec.get_text(url)\n",
    "\n",
    "    tables = pd.read_html(html)\n",
    "    # keep statement-like tables\n",
    "    keep = [t for t in tables if t.shape[1] >= 2 and t.shape[0] >= 6]\n",
    "\n",
    "    cleaned = [clean_statement_table(t) for t in keep]\n",
    "\n",
    "    best_idx = None\n",
    "    best_score = -10**9\n",
    "    for i, t in enumerate(cleaned):\n",
    "        s = table_score(kind, t)\n",
    "        if s > best_score:\n",
    "            best_score = s\n",
    "            best_idx = i\n",
    "\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"all_tables\": cleaned,\n",
    "        \"best_table_index\": best_idx,\n",
    "        \"best_table\": cleaned[best_idx] if (best_idx is not None and cleaned) else None\n",
    "    }\n",
    "\n",
    "def get_3_statements(sec: SECClient, filing: FilingRef) -> Dict[str, Optional[Dict]]:\n",
    "    files = list_filing_files(sec, filing)\n",
    "    if \"FilingSummary.xml\" not in files:\n",
    "        # This happens for some filings; robust fallback would parse the primary doc.\n",
    "        # For now, fail loudly so you know why it didn’t work.\n",
    "        raise RuntimeError(\"FilingSummary.xml not found in filing folder. Fallback parsing not implemented in this version.\")\n",
    "\n",
    "    fs_xml = sec.get_text(filing.archive_dir + \"FilingSummary.xml\")\n",
    "    reports = parse_filing_summary_xml(fs_xml)\n",
    "\n",
    "    bs_rep = pick_best_report(reports, \"BS\")\n",
    "    is_rep = pick_best_report(reports, \"IS\")\n",
    "    cfs_rep = pick_best_report(reports, \"CFS\")\n",
    "\n",
    "    out = {\"BS\": None, \"IS\": None, \"CFS\": None}\n",
    "\n",
    "    if bs_rep:\n",
    "        out[\"BS\"] = {\"report_name\": bs_rep[\"short\"] or bs_rep[\"long\"],\n",
    "                     **extract_statement_tables(sec, filing, bs_rep[\"html\"], \"BS\")}\n",
    "    if is_rep:\n",
    "        out[\"IS\"] = {\"report_name\": is_rep[\"short\"] or is_rep[\"long\"],\n",
    "                     **extract_statement_tables(sec, filing, is_rep[\"html\"], \"IS\")}\n",
    "    if cfs_rep:\n",
    "        out[\"CFS\"] = {\"report_name\": cfs_rep[\"short\"] or cfs_rep[\"long\"],\n",
    "                      **extract_statement_tables(sec, filing, cfs_rep[\"html\"], \"CFS\")}\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT\n",
    "# ============================================================\n",
    "def safe_sheet(name: str) -> str:\n",
    "    # Excel tab limit: 31 chars\n",
    "    return re.sub(r\"[\\[\\]\\:\\*\\?\\/\\\\]\", \"_\", name)[:31]\n",
    "\n",
    "def write_filing_to_excel(stmts: Dict[str, Optional[Dict]], out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
    "        for kind in [\"BS\", \"IS\", \"CFS\"]:\n",
    "            obj = stmts.get(kind)\n",
    "            if not obj or obj.get(\"best_table\") is None:\n",
    "                continue\n",
    "\n",
    "            # best table to main sheet\n",
    "            obj[\"best_table\"].to_excel(writer, sheet_name=kind, index=False)\n",
    "\n",
    "            # extras\n",
    "            for i, df in enumerate(obj.get(\"all_tables\", []), start=1):\n",
    "                if i - 1 == obj.get(\"best_table_index\"):\n",
    "                    continue\n",
    "                df.to_excel(writer, sheet_name=safe_sheet(f\"{kind}_{i}\"), index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# INTERACTIVE RUNNER\n",
    "# ============================================================\n",
    "def interactive():\n",
    "    print(\"SEC 10-Q/10-K -> 3-statement extractor (Balance Sheet, Income Statement, Cash Flow)\")\n",
    "    ticker = input(\"Ticker (e.g., AAPL): \").strip().upper()\n",
    "    form = input(\"Filing type (10-K or 10-Q): \").strip().upper()\n",
    "    if form not in {\"10-K\", \"10-Q\"}:\n",
    "        raise ValueError(\"Filing type must be '10-K' or '10-Q'.\")\n",
    "\n",
    "    mode = input(\"Mode (latest/all): \").strip().lower()\n",
    "    if mode not in {\"latest\", \"all\"}:\n",
    "        raise ValueError(\"Mode must be 'latest' or 'all'.\")\n",
    "\n",
    "    include_amend = input(\"Include amendments (/A)? (y/n): \").strip().lower() == \"y\"\n",
    "    max_filings = 1\n",
    "    if mode == \"all\":\n",
    "        max_filings = int(input(\"Max number of filings to pull (e.g., 5, 10, 20): \").strip())\n",
    "\n",
    "    sec = SECClient(USER_AGENT, MIN_SECONDS_BETWEEN_REQUESTS)\n",
    "\n",
    "    ticker_u, cik, company, filings = collect_filings_for_form(\n",
    "        sec=sec,\n",
    "        ticker=ticker,\n",
    "        form_base=form,\n",
    "        mode=mode,\n",
    "        include_amendments=include_amend,\n",
    "        max_filings=max_filings\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCompany: {company} | Ticker: {ticker_u} | CIK: {cik}\")\n",
    "    print(f\"Found {len(filings)} filing(s) for {form} (mode={mode}).\")\n",
    "\n",
    "    # Create just one folder named with the ticker\n",
    "    ticker_dir = BASE_OUTPUT_DIR / ticker_u\n",
    "    ticker_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for n, f in enumerate(filings, start=1):\n",
    "        print(f\"\\n[{n}/{len(filings)}] {f.form} filed {f.filing_date} | accession {f.accession_number}\")\n",
    "        print(f\"Filing folder: {f.archive_dir}\")\n",
    "\n",
    "        try:\n",
    "            stmts = get_3_statements(sec, f)\n",
    "\n",
    "            # Save directly inside the ticker folder\n",
    "            out_file = ticker_dir / f\"{ticker_u}_{f.form}_{f.filing_date}_{f.accession_number}.xlsx\"\n",
    "            write_filing_to_excel(stmts, out_file)\n",
    "            print(f\"Saved: {out_file}\")\n",
    "\n",
    "            for kind in [\"BS\", \"IS\", \"CFS\"]:\n",
    "                obj = stmts.get(kind)\n",
    "                if obj and obj.get(\"best_table\") is not None:\n",
    "                    print(f\"  {kind}: {obj['report_name']} | {len(obj.get('all_tables', []))} table(s) | {obj['url']}\")\n",
    "                else:\n",
    "                    print(f\"  {kind}: NOT FOUND\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FAILED for {f.accession_number}: {e}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "\n",
    "# Run interactively (works in notebook too)\n",
    "interactive()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80022155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discovering latest SEC 13F dataset links...\n",
      "\n",
      "Choose dataset:\n",
      "   1) 2025 September October November 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01sep2025-30nov2025_form13f.zip\n",
      "   2) 2025 June July August 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01jun2025-31aug2025_form13f.zip\n",
      "   3) 2025 March April May 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01mar2025-31may2025_form13f.zip\n",
      "   4) 2024 December 2025 January February 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01dec2024-28feb2025_form13f.zip\n",
      "   5) 2024 September October November 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01sep2024-30nov2024_form13f.zip\n",
      "   6) 2024 June July August 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01jun2024-31aug2024_form13f.zip\n",
      "   7) 2024 March April May 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01mar2024-31may2024_form13f.zip\n",
      "   8) 2024 January February 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/01jan2024-29feb2024_form13f.zip\n",
      "   9) 2023 Q4 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/2023q4_form13f.zip\n",
      "  10) 2023 Q3 13F  |  https://www.sec.gov/files/structureddata/data/form-13f-data-sets/2023q3_form13f.zip\n",
      "\n",
      "Selected: 2025 September October November 13F\n",
      "\n",
      "Downloading ZIP to: C:\\Users\\reigh\\Desktop\\Fin Statements\\13F_Bulk_Overlap\\dataset_form13f.zip\n",
      "Extracting ZIP to: C:\\Users\\reigh\\Desktop\\Fin Statements\\13F_Bulk_Overlap\\raw_extracted\n",
      "\n",
      "Loading SUBMISSION: SUBMISSION.tsv\n",
      "Selected 3 accession(s) = last 2 period(s) per manager where available.\n",
      "Loading COVERPAGE: COVERPAGE.tsv\n",
      "Loading INFOTABLE (chunked): INFOTABLE.tsv\n",
      "\n",
      "Saved: C:\\Users\\reigh\\Desktop\\Fin Statements\\13F_Bulk_Overlap\\13F_bulk_overlap_summary.xlsx\n",
      "\n",
      "Use these tabs for change detection:\n",
      " - fund_changes_long: NEW / EXIT / INCREASE / DECREASE / UNCHANGED per CUSIP per manager.\n",
      " - fund_changes_summary: counts + aggregate deltas per manager (latest vs previous).\n",
      " - periods_selected: exactly which two report periods were pulled per CIK.\n",
      "\n",
      "If the file name has a timestamp: you had the old workbook open in Excel.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "USER_AGENT = \"Name (your.email@domain.com) - 13F overlap research\"  # must include '@'\n",
    "MIN_SECONDS_BETWEEN_REQUESTS = 0.22\n",
    "\n",
    "BASE_OUTPUT_DIR = Path(r\"C:\\Users\\reigh\\Desktop\\Fin Statements\")\n",
    "OUT_FOLDER_NAME = \"13F_Bulk_Overlap\"\n",
    "\n",
    "# Overlap behavior\n",
    "DISTINGUISH_OPTIONS = False          # if True: overlap key = CUSIP|PUTCALL (usually not recommended)\n",
    "EXCLUDE_OPTIONS_FROM_OVERLAP = True  # if True: ignore rows with PUTCALL for overlap + weights/totals\n",
    "\n",
    "# Overlap definition (per period)\n",
    "OVERLAP_MIN_MANAGERS = 2\n",
    "\n",
    "# Auto-change analysis\n",
    "AUTO_USE_LAST_N_PERIODS_PER_MANAGER = 2   # <-- key: last 2 reporting periods per manager\n",
    "\n",
    "# Idea-generation thresholds (tuned for 5–10 CIKs)\n",
    "HIGH_CONVICTION_MAX_WEIGHT_PCT = 3.0\n",
    "CROWDED_MIN_ABS = 3\n",
    "CROWDED_MIN_FRAC = 0.50\n",
    "\n",
    "# Change classification thresholds (to avoid noise)\n",
    "WEIGHT_CHANGE_PCT_THRESHOLD = 0.05  # 0.05% weight move treated as \"unchanged\" band\n",
    "\n",
    "SEC_13F_DATASETS_PAGE = \"https://www.sec.gov/data-research/sec-markets-data/form-13f-data-sets\"\n",
    "\n",
    "# Excel limits\n",
    "EXCEL_MAX_ROWS = 1_000_000\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SEC CLIENT\n",
    "# =========================\n",
    "class SECClient:\n",
    "    def __init__(self, user_agent: str, min_interval_s: float = 0.22):\n",
    "        if not user_agent or \"@\" not in user_agent:\n",
    "            raise ValueError(\"Set USER_AGENT like 'Name (your@email.com) - purpose'.\")\n",
    "        self.s = requests.Session()\n",
    "        self.s.headers.update({\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        })\n",
    "        self.min_interval_s = min_interval_s\n",
    "        self._last_request_ts = 0.0\n",
    "\n",
    "    def _sleep_if_needed(self):\n",
    "        now = time.time()\n",
    "        dt = now - self._last_request_ts\n",
    "        if dt < self.min_interval_s:\n",
    "            time.sleep(self.min_interval_s - dt)\n",
    "\n",
    "    def get(self, url: str, stream: bool = False, max_retries: int = 6) -> requests.Response:\n",
    "        for attempt in range(max_retries):\n",
    "            self._sleep_if_needed()\n",
    "            r = self.s.get(url, timeout=60, stream=stream)\n",
    "            self._last_request_ts = time.time()\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(min(2 ** attempt, 16))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "        raise RuntimeError(f\"Failed GET: {url}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_cik(x: str) -> str:\n",
    "    s = re.sub(r\"\\D\", \"\", x.strip())\n",
    "    if not s:\n",
    "        raise ValueError(f\"Bad CIK: {x}\")\n",
    "    return str(int(s)).zfill(10)\n",
    "\n",
    "def parse_dd_mon_yyyy(s: str) -> pd.Timestamp:\n",
    "    return pd.to_datetime(s, format=\"%d-%b-%Y\", errors=\"coerce\")\n",
    "\n",
    "def holding_key(df: pd.DataFrame) -> pd.Series:\n",
    "    if DISTINGUISH_OPTIONS:\n",
    "        return df[\"CUSIP\"].astype(str) + \"|\" + df[\"PUTCALL\"].fillna(\"\").astype(str)\n",
    "    return df[\"CUSIP\"].astype(str)\n",
    "\n",
    "def find_file_by_token(folder: Path, token: str) -> Path:\n",
    "    token_u = token.upper()\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and token_u in p.name.upper() and p.suffix.lower() in {\".tsv\", \".txt\"}:\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Could not find a TSV/TXT file containing '{token}' under {folder}\")\n",
    "\n",
    "def compute_pairwise(keys_by_mgr: Dict[str, set]) -> pd.DataFrame:\n",
    "    mgrs = sorted(keys_by_mgr.keys())\n",
    "    rows = []\n",
    "    for i in range(len(mgrs)):\n",
    "        for j in range(i + 1, len(mgrs)):\n",
    "            a, b = mgrs[i], mgrs[j]\n",
    "            sa, sb = keys_by_mgr[a], keys_by_mgr[b]\n",
    "            inter = len(sa & sb)\n",
    "            union = len(sa | sb)\n",
    "            rows.append({\n",
    "                \"ManagerA\": a,\n",
    "                \"ManagerB\": b,\n",
    "                \"OverlapCount\": inter,\n",
    "                \"UnionCount\": union,\n",
    "                \"Jaccard\": (inter / union) if union else 0.0\n",
    "            })\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"ManagerA\", \"ManagerB\", \"OverlapCount\", \"UnionCount\", \"Jaccard\"])\n",
    "    return pd.DataFrame(rows).sort_values([\"OverlapCount\", \"Jaccard\"], ascending=False)\n",
    "\n",
    "def safe_excel_writer_path(out_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    If the file is open in Excel, write to a timestamped filename instead.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base_path = out_dir / f\"{base_name}.xlsx\"\n",
    "    try:\n",
    "        with open(base_path, \"ab\"):\n",
    "            pass\n",
    "        return base_path\n",
    "    except PermissionError:\n",
    "        ts = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return out_dir / f\"{base_name}_{ts}.xlsx\"\n",
    "\n",
    "def clean_sheet_name(s: str) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9 _-]\", \"\", str(s)).strip()\n",
    "    return (s[:31] or \"sheet\")\n",
    "\n",
    "def write_df_split(writer: pd.ExcelWriter, df: pd.DataFrame, sheet_base: str, max_rows: int = EXCEL_MAX_ROWS) -> None:\n",
    "    sheet_base = clean_sheet_name(sheet_base)\n",
    "    if df is None or df.empty:\n",
    "        pd.DataFrame().to_excel(writer, sheet_name=sheet_base[:31], index=False)\n",
    "        return\n",
    "\n",
    "    n = len(df)\n",
    "    if n <= max_rows:\n",
    "        df.to_excel(writer, sheet_name=sheet_base[:31], index=False)\n",
    "        return\n",
    "\n",
    "    k = 0\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        k += 1\n",
    "        chunk = df.iloc[start:start + max_rows].copy()\n",
    "        suffix = f\"_{k}\"\n",
    "        name = (sheet_base[: (31 - len(suffix))] + suffix)[:31]\n",
    "        chunk.to_excel(writer, sheet_name=name, index=False)\n",
    "        start += max_rows\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DISCOVER + DOWNLOAD DATASET ZIP\n",
    "# =========================\n",
    "def list_dataset_links(sec: SECClient, max_items: int = 12) -> List[Tuple[str, str]]:\n",
    "    html = sec.get(SEC_13F_DATASETS_PAGE).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        text = a.get_text(\" \", strip=True)\n",
    "        if href.endswith(\"_form13f.zip\"):\n",
    "            if href.startswith(\"/\"):\n",
    "                href = \"https://www.sec.gov\" + href\n",
    "            links.append((text, href))\n",
    "    return links[:max_items]\n",
    "\n",
    "def download_zip(sec: SECClient, url: str, out_path: Path) -> None:\n",
    "    r = sec.get(url, stream=True)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD + FILTER TABLES\n",
    "# =========================\n",
    "def load_submission(submission_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(submission_path, sep=\"\\t\", dtype=str)\n",
    "    df[\"FILING_DATE_TS\"] = df[\"FILING_DATE\"].apply(parse_dd_mon_yyyy)\n",
    "    df[\"PERIODOFREPORT_TS\"] = df[\"PERIODOFREPORT\"].apply(parse_dd_mon_yyyy)\n",
    "    df[\"CIK\"] = df[\"CIK\"].astype(str).str.zfill(10)\n",
    "    return df\n",
    "\n",
    "def pick_latest_accessions_per_period(sub: pd.DataFrame, target_ciks: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per (CIK, PERIODOFREPORT) selecting latest FILING_DATE (so amendment wins).\n",
    "    \"\"\"\n",
    "    sub = sub[sub[\"SUBMISSIONTYPE\"].isin([\"13F-HR\", \"13F-HR/A\"])].copy()\n",
    "    sub = sub[sub[\"CIK\"].isin(target_ciks)].copy()\n",
    "\n",
    "    sub = sub.sort_values([\"CIK\", \"PERIODOFREPORT_TS\", \"FILING_DATE_TS\"], ascending=[True, True, False])\n",
    "    picked = sub.drop_duplicates(subset=[\"CIK\", \"PERIODOFREPORT_TS\"], keep=\"first\")\n",
    "    return picked[[\n",
    "        \"ACCESSION_NUMBER\", \"CIK\", \"SUBMISSIONTYPE\", \"FILING_DATE\", \"PERIODOFREPORT\",\n",
    "        \"FILING_DATE_TS\", \"PERIODOFREPORT_TS\"\n",
    "    ]].copy()\n",
    "\n",
    "def pick_last_n_periods_per_manager(picked_per_period: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each CIK, select last n reporting periods (most recent PERIODOFREPORT_TS).\n",
    "    \"\"\"\n",
    "    out = (\n",
    "        picked_per_period.sort_values([\"CIK\", \"PERIODOFREPORT_TS\"], ascending=[True, False])\n",
    "        .groupby(\"CIK\", dropna=False)\n",
    "        .head(n)\n",
    "        .copy()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def load_coverpage(coverpage_path: Path, accessions: set) -> pd.DataFrame:\n",
    "    df = pd.read_csv(coverpage_path, sep=\"\\t\", dtype=str)\n",
    "    df = df[df[\"ACCESSION_NUMBER\"].isin(accessions)].copy()\n",
    "    return df\n",
    "\n",
    "def load_infotable_chunked(infotable_path: Path, accessions: set) -> pd.DataFrame:\n",
    "    usecols = [\n",
    "        \"ACCESSION_NUMBER\", \"INFOTABLE_SK\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "        \"CUSIP\", \"FIGI\", \"VALUE\", \"SSHPRNAMT\", \"SSHPRNAMTTYPE\", \"PUTCALL\",\n",
    "        \"INVESTMENTDISCRETION\", \"OTHERMANAGER\", \"VOTING_AUTH_SOLE\", \"VOTING_AUTH_SHARED\", \"VOTING_AUTH_NONE\"\n",
    "    ]\n",
    "    chunks = []\n",
    "    for ch in pd.read_csv(infotable_path, sep=\"\\t\", dtype=str, usecols=lambda c: c in usecols, chunksize=400_000):\n",
    "        ch = ch[ch[\"ACCESSION_NUMBER\"].isin(accessions)]\n",
    "        if not ch.empty:\n",
    "            chunks.append(ch)\n",
    "    if not chunks:\n",
    "        return pd.DataFrame(columns=usecols)\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    df[\"CUSIP\"] = df[\"CUSIP\"].astype(str).str.replace(r\"\\s+\", \"\", regex=True)\n",
    "\n",
    "    for c in [\"VALUE\", \"SSHPRNAMT\", \"VOTING_AUTH_SOLE\", \"VOTING_AUTH_SHARED\", \"VOTING_AUTH_NONE\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "\n",
    "    if \"PUTCALL\" in df.columns:\n",
    "        df[\"PUTCALL\"] = df[\"PUTCALL\"].replace(\"\", pd.NA)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CHANGE ANALYSIS\n",
    "# =========================\n",
    "def build_fund_changes(mgr_cusip_vals: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each manager, compare latest period vs previous period within the selected data.\n",
    "    Returns:\n",
    "      - fund_changes_long: one row per CUSIP with NEW/EXIT/INCREASE/DECREASE/UNCHANGED\n",
    "      - fund_changes_summary: manager-level summary\n",
    "    \"\"\"\n",
    "    if mgr_cusip_vals.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Determine last 2 periods per manager from available rows (already filtered earlier)\n",
    "    periods_by_mgr = (\n",
    "        mgr_cusip_vals[[\"manager\", \"PERIODOFREPORT_TS\"]].drop_duplicates()\n",
    "        .sort_values([\"manager\", \"PERIODOFREPORT_TS\"], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "    mgr_to_two = (\n",
    "        periods_by_mgr.groupby(\"manager\", dropna=False)[\"PERIODOFREPORT_TS\"]\n",
    "        .apply(lambda s: list(s.head(2)))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for mgr, per_list in mgr_to_two.items():\n",
    "        if len(per_list) < 2:\n",
    "            continue\n",
    "        p_curr_ts, p_prev_ts = per_list[0], per_list[1]\n",
    "\n",
    "        curr = mgr_cusip_vals[(mgr_cusip_vals[\"manager\"] == mgr) & (mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == p_curr_ts)].copy()\n",
    "        prev = mgr_cusip_vals[(mgr_cusip_vals[\"manager\"] == mgr) & (mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == p_prev_ts)].copy()\n",
    "\n",
    "        # Outer join on CUSIP\n",
    "        j = curr.merge(\n",
    "            prev,\n",
    "            on=[\"manager\", \"CUSIP\"],\n",
    "            how=\"outer\",\n",
    "            suffixes=(\"_curr\", \"_prev\")\n",
    "        )\n",
    "\n",
    "        # Carry period labels\n",
    "        j[\"PERIOD_CURR_TS\"] = p_curr_ts\n",
    "        j[\"PERIOD_PREV_TS\"] = p_prev_ts\n",
    "\n",
    "        # Prefer current labels else previous\n",
    "        for col in [\"PERIODOFREPORT\", \"NAMEOFISSUER\", \"TITLEOFCLASS\"]:\n",
    "            j[col] = j[f\"{col}_curr\"].combine_first(j[f\"{col}_prev\"])\n",
    "\n",
    "        # Fill missing numeric values\n",
    "        for col in [\"PositionValue_AsFiled\", \"Total13FValue_AsFiled\", \"WeightPct\"]:\n",
    "            j[f\"{col}_curr\"] = j[f\"{col}_curr\"].fillna(0)\n",
    "            j[f\"{col}_prev\"] = j[f\"{col}_prev\"].fillna(0)\n",
    "\n",
    "        j[\"DeltaValue_AsFiled\"] = j[\"PositionValue_AsFiled_curr\"] - j[\"PositionValue_AsFiled_prev\"]\n",
    "        j[\"DeltaWeightPct\"] = j[\"WeightPct_curr\"] - j[\"WeightPct_prev\"]\n",
    "\n",
    "        def classify(r):\n",
    "            had_prev = r[\"PositionValue_AsFiled_prev\"] > 0\n",
    "            has_curr = r[\"PositionValue_AsFiled_curr\"] > 0\n",
    "            if (not had_prev) and has_curr:\n",
    "                return \"NEW\"\n",
    "            if had_prev and (not has_curr):\n",
    "                return \"EXIT\"\n",
    "            # both present\n",
    "            if abs(r[\"DeltaWeightPct\"]) < WEIGHT_CHANGE_PCT_THRESHOLD:\n",
    "                return \"UNCHANGED\"\n",
    "            return \"INCREASE\" if r[\"DeltaWeightPct\"] > 0 else \"DECREASE\"\n",
    "\n",
    "        j[\"ChangeType\"] = j.apply(classify, axis=1)\n",
    "\n",
    "        out_cols = [\n",
    "            \"manager\",\n",
    "            \"PERIODOFREPORT\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "            \"PERIOD_PREV_TS\", \"PERIOD_CURR_TS\",\n",
    "            \"PositionValue_AsFiled_prev\", \"WeightPct_prev\",\n",
    "            \"PositionValue_AsFiled_curr\", \"WeightPct_curr\",\n",
    "            \"DeltaValue_AsFiled\", \"DeltaWeightPct\",\n",
    "            \"ChangeType\"\n",
    "        ]\n",
    "        rows.append(j[out_cols])\n",
    "\n",
    "    fund_changes_long = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "    if fund_changes_long.empty:\n",
    "        return fund_changes_long, pd.DataFrame()\n",
    "\n",
    "    # Human-readable period strings\n",
    "    fund_changes_long[\"PERIOD_PREV\"] = fund_changes_long[\"PERIOD_PREV_TS\"].dt.strftime(\"%d-%b-%Y\")\n",
    "    fund_changes_long[\"PERIOD_CURR\"] = fund_changes_long[\"PERIOD_CURR_TS\"].dt.strftime(\"%d-%b-%Y\")\n",
    "\n",
    "    # Summary per manager\n",
    "    summ = (\n",
    "        fund_changes_long.groupby([\"manager\", \"PERIOD_PREV\", \"PERIOD_CURR\"], dropna=False)\n",
    "        .agg(\n",
    "            NewPositions=(\"ChangeType\", lambda s: (s == \"NEW\").sum()),\n",
    "            Exits=(\"ChangeType\", lambda s: (s == \"EXIT\").sum()),\n",
    "            Increases=(\"ChangeType\", lambda s: (s == \"INCREASE\").sum()),\n",
    "            Decreases=(\"ChangeType\", lambda s: (s == \"DECREASE\").sum()),\n",
    "            Unchanged=(\"ChangeType\", lambda s: (s == \"UNCHANGED\").sum()),\n",
    "            SumDeltaWeightPct=(\"DeltaWeightPct\", \"sum\"),\n",
    "            SumAbsDeltaWeightPct=(\"DeltaWeightPct\", lambda x: x.abs().sum()),\n",
    "            SumDeltaValue_AsFiled=(\"DeltaValue_AsFiled\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"SumAbsDeltaWeightPct\"], ascending=[False])\n",
    "    )\n",
    "\n",
    "    # Sort long table for usability\n",
    "    fund_changes_long = fund_changes_long.sort_values(\n",
    "        [\"manager\", \"ChangeType\", \"DeltaWeightPct\"],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "\n",
    "    return fund_changes_long, summ\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def interactive_bulk_13f():\n",
    "    sec = SECClient(USER_AGENT, MIN_SECONDS_BETWEEN_REQUESTS)\n",
    "\n",
    "    cik_input = input(\"Enter manager CIKs (comma-separated): \").strip()\n",
    "    target_ciks = [normalize_cik(x) for x in cik_input.split(\",\") if x.strip()]\n",
    "    if len(target_ciks) < 2:\n",
    "        raise ValueError(\"Provide at least 2 CIKs to compute overlaps.\")\n",
    "\n",
    "    print(\"\\nDiscovering latest SEC 13F dataset links...\")\n",
    "    links = list_dataset_links(sec, max_items=10)\n",
    "    if not links:\n",
    "        raise RuntimeError(\"Could not find dataset links on the SEC page.\")\n",
    "\n",
    "    print(\"\\nChoose dataset:\")\n",
    "    for i, (label, url) in enumerate(links, start=1):\n",
    "        print(f\"  {i:2d}) {label}  |  {url}\")\n",
    "\n",
    "    choice = input(\"\\nEnter number (or press Enter for #1 = latest): \").strip()\n",
    "    idx = 1 if choice == \"\" else int(choice)\n",
    "    if idx < 1 or idx > len(links):\n",
    "        raise ValueError(\"Invalid selection.\")\n",
    "    label, zip_url = links[idx - 1]\n",
    "    print(f\"\\nSelected: {label}\")\n",
    "\n",
    "    out_dir = BASE_OUTPUT_DIR / OUT_FOLDER_NAME\n",
    "    raw_dir = out_dir / \"raw_extracted\"\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    zip_path = out_dir / \"dataset_form13f.zip\"\n",
    "    print(f\"\\nDownloading ZIP to: {zip_path}\")\n",
    "    download_zip(sec, zip_url, zip_path)\n",
    "\n",
    "    print(f\"Extracting ZIP to: {raw_dir}\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(raw_dir)\n",
    "\n",
    "    submission_path = find_file_by_token(raw_dir, \"SUBMISSION\")\n",
    "    infotable_path = find_file_by_token(raw_dir, \"INFOTABLE\")\n",
    "    coverpage_path = None\n",
    "    try:\n",
    "        coverpage_path = find_file_by_token(raw_dir, \"COVERPAGE\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\nLoading SUBMISSION: {submission_path.name}\")\n",
    "    sub = load_submission(submission_path)\n",
    "\n",
    "    # 1) pick latest filing per (CIK, period)\n",
    "    picked_pp = pick_latest_accessions_per_period(sub, target_ciks)\n",
    "    if picked_pp.empty:\n",
    "        raise RuntimeError(\"No 13F-HR/13F-HR/A rows found for your CIKs in this dataset.\")\n",
    "\n",
    "    # 2) then pick last N periods per manager (CIK)\n",
    "    picked = pick_last_n_periods_per_manager(picked_pp, AUTO_USE_LAST_N_PERIODS_PER_MANAGER)\n",
    "\n",
    "    # If a manager has only 1 period in dataset, it will still remain; changes will skip it.\n",
    "    accessions = set(picked[\"ACCESSION_NUMBER\"].tolist())\n",
    "    print(f\"Selected {len(accessions)} accession(s) = last {AUTO_USE_LAST_N_PERIODS_PER_MANAGER} period(s) per manager where available.\")\n",
    "\n",
    "    cover = pd.DataFrame()\n",
    "    if coverpage_path is not None:\n",
    "        print(f\"Loading COVERPAGE: {coverpage_path.name}\")\n",
    "        cover = load_coverpage(coverpage_path, accessions)\n",
    "\n",
    "    print(f\"Loading INFOTABLE (chunked): {infotable_path.name}\")\n",
    "    info = load_infotable_chunked(infotable_path, accessions)\n",
    "    if info.empty:\n",
    "        raise RuntimeError(\"INFOTABLE filter returned 0 rows — check if dataset includes holdings for your CIKs.\")\n",
    "\n",
    "    merged = info.merge(\n",
    "        picked[[\"ACCESSION_NUMBER\", \"CIK\", \"SUBMISSIONTYPE\", \"FILING_DATE\", \"PERIODOFREPORT\", \"FILING_DATE_TS\", \"PERIODOFREPORT_TS\"]],\n",
    "        on=\"ACCESSION_NUMBER\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    if not cover.empty and \"FILINGMANAGER_NAME\" in cover.columns:\n",
    "        merged = merged.merge(\n",
    "            cover[[\"ACCESSION_NUMBER\", \"FILINGMANAGER_NAME\", \"ISAMENDMENT\", \"AMENDMENTTYPE\"]],\n",
    "            on=\"ACCESSION_NUMBER\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        merged[\"FILINGMANAGER_NAME\"] = None\n",
    "        merged[\"ISAMENDMENT\"] = None\n",
    "        merged[\"AMENDMENTTYPE\"] = None\n",
    "\n",
    "    merged[\"manager\"] = merged.apply(\n",
    "        lambda r: (r[\"FILINGMANAGER_NAME\"] if isinstance(r[\"FILINGMANAGER_NAME\"], str) and r[\"FILINGMANAGER_NAME\"].strip()\n",
    "                   else f\"CIK{str(r['CIK']).zfill(10)}\"),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Base universe (option filter)\n",
    "    base = merged.copy()\n",
    "    if EXCLUDE_OPTIONS_FROM_OVERLAP and \"PUTCALL\" in base.columns:\n",
    "        base = base[base[\"PUTCALL\"].isna()]\n",
    "\n",
    "    # Fund totals + weights (as filed unit cancels in weights)\n",
    "    mgr_cusip_vals = (\n",
    "        base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\", \"CUSIP\"], dropna=False)[\"VALUE\"]\n",
    "        .sum(min_count=1)\n",
    "        .rename(\"PositionValue_AsFiled\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    fund_totals = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\"], dropna=False)[\"PositionValue_AsFiled\"]\n",
    "        .sum(min_count=1)\n",
    "        .rename(\"Total13FValue_AsFiled\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    mgr_cusip_vals = mgr_cusip_vals.merge(\n",
    "        fund_totals, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\"], how=\"left\"\n",
    "    )\n",
    "    mgr_cusip_vals[\"WeightPct\"] = (mgr_cusip_vals[\"PositionValue_AsFiled\"] / mgr_cusip_vals[\"Total13FValue_AsFiled\"]) * 100\n",
    "\n",
    "    labels = (\n",
    "        base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[[\"NAMEOFISSUER\", \"TITLEOFCLASS\"]]\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "    mgr_cusip_vals = mgr_cusip_vals.merge(labels, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "\n",
    "    # -------------------------\n",
    "    # OVERLAPS (per period)\n",
    "    # -------------------------\n",
    "    cusip_mgr_counts = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"manager\"]\n",
    "        .nunique()\n",
    "        .rename(\"ManagersHolding\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    overlap_cusips = cusip_mgr_counts[cusip_mgr_counts[\"ManagersHolding\"] >= OVERLAP_MIN_MANAGERS].copy()\n",
    "\n",
    "    cusip_total_value = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"PositionValue_AsFiled\"]\n",
    "        .sum(min_count=1)\n",
    "        .rename(\"TotalValue_AsFiled_AllManagers\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    all_overlaps = (\n",
    "        overlap_cusips.merge(cusip_total_value, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "        .merge(labels, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "        .sort_values([\"PERIODOFREPORT_TS\", \"ManagersHolding\", \"TotalValue_AsFiled_AllManagers\"],\n",
    "                     ascending=[False, False, False])\n",
    "    )\n",
    "\n",
    "    overlap_weights_long = (\n",
    "        mgr_cusip_vals.merge(overlap_cusips, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"inner\")\n",
    "        .sort_values([\"PERIODOFREPORT_TS\", \"ManagersHolding\", \"CUSIP\", \"WeightPct\"],\n",
    "                     ascending=[False, False, True, False])\n",
    "    )\n",
    "\n",
    "    overlap_weights_matrix = overlap_weights_long.pivot_table(\n",
    "        index=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\", \"ManagersHolding\"],\n",
    "        columns=\"manager\",\n",
    "        values=\"WeightPct\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Pairwise overlap counts per period\n",
    "    pairwise_frames = []\n",
    "    for (period, period_ts), dfp in base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\"], dropna=False):\n",
    "        keys_by_mgr = {}\n",
    "        for mgr, dfm in dfp.groupby(\"manager\"):\n",
    "            keys_by_mgr[mgr] = set(holding_key(dfm).dropna().tolist())\n",
    "        pw = compute_pairwise(keys_by_mgr)\n",
    "        if not pw.empty:\n",
    "            pw.insert(0, \"PERIODOFREPORT\", period)\n",
    "            pw.insert(1, \"PERIODOFREPORT_TS\", period_ts)\n",
    "            pairwise_frames.append(pw)\n",
    "\n",
    "    pairwise_overlap = pd.concat(pairwise_frames, ignore_index=True) if pairwise_frames else pd.DataFrame(\n",
    "        columns=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"ManagerA\", \"ManagerB\", \"OverlapCount\", \"UnionCount\", \"Jaccard\"]\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # COMMON ALL (per period)\n",
    "    # -------------------------\n",
    "    common_all_rows = []\n",
    "    for (period, period_ts), dfp in base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\"], dropna=False):\n",
    "        sets = []\n",
    "        for _, g in dfp.groupby(\"manager\"):\n",
    "            sets.append(set(g[\"CUSIP\"].dropna().astype(str).tolist()))\n",
    "        common_cusips = set.intersection(*sets) if sets else set()\n",
    "        for cusip in common_cusips:\n",
    "            common_all_rows.append({\"PERIODOFREPORT\": period, \"PERIODOFREPORT_TS\": period_ts, \"CUSIP\": cusip})\n",
    "\n",
    "    common_all_universe = pd.DataFrame(common_all_rows)\n",
    "    if common_all_universe.empty:\n",
    "        common_all = pd.DataFrame(columns=[\n",
    "            \"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "            \"PositionValue_AsFiled\", \"Total13FValue_AsFiled\", \"WeightPct\", \"ManagersHolding\"\n",
    "        ])\n",
    "    else:\n",
    "        common_all = (\n",
    "            mgr_cusip_vals.merge(common_all_universe, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"inner\")\n",
    "            .merge(cusip_mgr_counts, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "            .sort_values([\"PERIODOFREPORT_TS\", \"WeightPct\"], ascending=[False, False])\n",
    "        )\n",
    "\n",
    "    # ============================================================\n",
    "    # ADDITIONS 1–4 (idea generation)\n",
    "    # ============================================================\n",
    "    sec_stats = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\"], dropna=False)\n",
    "        .agg(\n",
    "            ManagersHolding=(\"manager\", \"nunique\"),\n",
    "            TotalValue_AllManagers_AsFiled=(\"PositionValue_AsFiled\", \"sum\"),\n",
    "            SumWeightPct=(\"WeightPct\", \"sum\"),\n",
    "            AvgWeightPct=(\"WeightPct\", \"mean\"),\n",
    "            MedianWeightPct=(\"WeightPct\", \"median\"),\n",
    "            MaxWeightPct=(\"WeightPct\", \"max\"),\n",
    "            MinWeightPct=(\"WeightPct\", \"min\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    top_holder = (\n",
    "        mgr_cusip_vals.sort_values([\"PERIODOFREPORT_TS\", \"CUSIP\", \"WeightPct\"], ascending=[False, True, False])\n",
    "        .groupby([\"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)\n",
    "        .head(1)[[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"manager\", \"WeightPct\"]]\n",
    "        .rename(columns={\"manager\": \"TopHolder\", \"WeightPct\": \"TopHolderWeightPct\"})\n",
    "    )\n",
    "    sec_stats = sec_stats.merge(top_holder, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "\n",
    "    tmp = mgr_cusip_vals.copy()\n",
    "    tmp[\"SumWeight_forCUSIP\"] = tmp.groupby([\"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"WeightPct\"].transform(\"sum\")\n",
    "    tmp[\"w_norm\"] = tmp[\"WeightPct\"] / tmp[\"SumWeight_forCUSIP\"]\n",
    "    hhi = (\n",
    "        tmp.groupby([\"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"w_norm\"]\n",
    "        .apply(lambda s: (s.fillna(0) ** 2).sum())\n",
    "        .rename(\"WeightHHI\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    sec_stats = sec_stats.merge(hhi, on=[\"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "\n",
    "    sec_stats[\"ConsensusScore\"] = (\n",
    "        (sec_stats[\"ManagersHolding\"] ** 0.5) * (sec_stats[\"AvgWeightPct\"] + 0.5)\n",
    "        + (sec_stats[\"SumWeightPct\"] * 0.20)\n",
    "    )\n",
    "    sec_stats[\"NicheConvictionScore\"] = (\n",
    "        (sec_stats[\"MaxWeightPct\"] * 1.00) * (sec_stats[\"WeightHHI\"].fillna(0) + 0.10)\n",
    "    )\n",
    "\n",
    "    idea_ranker_overlaps = sec_stats[sec_stats[\"ManagersHolding\"] >= OVERLAP_MIN_MANAGERS].copy()\n",
    "    idea_ranker_overlaps = idea_ranker_overlaps.sort_values(\n",
    "        [\"PERIODOFREPORT_TS\", \"ConsensusScore\", \"NicheConvictionScore\"],\n",
    "        ascending=[False, False, False]\n",
    "    )\n",
    "\n",
    "    mgr_count_per_period = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\"], dropna=False)[\"manager\"]\n",
    "        .nunique().rename(\"ManagersInUniverse\").reset_index()\n",
    "    )\n",
    "    idea_ranker_overlaps = idea_ranker_overlaps.merge(mgr_count_per_period, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\"], how=\"left\")\n",
    "    idea_ranker_overlaps[\"CrowdedThreshold\"] = idea_ranker_overlaps[\"ManagersInUniverse\"].apply(\n",
    "        lambda n: max(CROWDED_MIN_ABS, int((n * CROWDED_MIN_FRAC) + 0.9999))\n",
    "    )\n",
    "\n",
    "    def bucket_row(r):\n",
    "        crowded = r[\"ManagersHolding\"] >= r[\"CrowdedThreshold\"]\n",
    "        high_conv = r[\"MaxWeightPct\"] >= HIGH_CONVICTION_MAX_WEIGHT_PCT\n",
    "        if crowded and high_conv:\n",
    "            return \"Consensus high conviction\"\n",
    "        if crowded and not high_conv:\n",
    "            return \"Crowded low conviction\"\n",
    "        if (not crowded) and high_conv:\n",
    "            return \"Niche high conviction\"\n",
    "        return \"Other\"\n",
    "\n",
    "    idea_ranker_overlaps[\"IdeaBucket\"] = idea_ranker_overlaps.apply(bucket_row, axis=1)\n",
    "\n",
    "    idea_quadrants = idea_ranker_overlaps[[\n",
    "        \"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "        \"ManagersHolding\", \"ManagersInUniverse\", \"CrowdedThreshold\",\n",
    "        \"AvgWeightPct\", \"MaxWeightPct\", \"SumWeightPct\",\n",
    "        \"TopHolder\", \"TopHolderWeightPct\",\n",
    "        \"WeightHHI\", \"ConsensusScore\", \"NicheConvictionScore\",\n",
    "        \"IdeaBucket\"\n",
    "    ]].copy()\n",
    "\n",
    "    # Weighted pairwise overlap by $ share of portfolio (per period)\n",
    "    holdings_set = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT_TS\", \"manager\"], dropna=False)[\"CUSIP\"]\n",
    "        .apply(lambda s: set(s.dropna().astype(str))).to_dict()\n",
    "    )\n",
    "    totals_map = (\n",
    "        fund_totals.set_index([\"PERIODOFREPORT_TS\", \"manager\"])[\"Total13FValue_AsFiled\"].to_dict()\n",
    "    )\n",
    "    pos_map = (\n",
    "        mgr_cusip_vals.set_index([\"PERIODOFREPORT_TS\", \"manager\", \"CUSIP\"])[\"PositionValue_AsFiled\"].to_dict()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for period_ts in sorted(mgr_cusip_vals[\"PERIODOFREPORT_TS\"].dropna().unique(), reverse=True):\n",
    "        mgrs = sorted(mgr_cusip_vals[mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == period_ts][\"manager\"].dropna().unique())\n",
    "        # recover period string\n",
    "        per_str = mgr_cusip_vals.loc[mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == period_ts, \"PERIODOFREPORT\"].iloc[0]\n",
    "        for i in range(len(mgrs)):\n",
    "            for j in range(i + 1, len(mgrs)):\n",
    "                a, b = mgrs[i], mgrs[j]\n",
    "                Sa = holdings_set.get((period_ts, a), set())\n",
    "                Sb = holdings_set.get((period_ts, b), set())\n",
    "                inter = Sa & Sb\n",
    "                if not inter:\n",
    "                    continue\n",
    "                a_total = totals_map.get((period_ts, a), 0) or 0\n",
    "                b_total = totals_map.get((period_ts, b), 0) or 0\n",
    "                a_overlap_val = sum(pos_map.get((period_ts, a, c), 0) or 0 for c in inter)\n",
    "                b_overlap_val = sum(pos_map.get((period_ts, b, c), 0) or 0 for c in inter)\n",
    "                rows.append({\n",
    "                    \"PERIODOFREPORT\": per_str,\n",
    "                    \"PERIODOFREPORT_TS\": period_ts,\n",
    "                    \"ManagerA\": a,\n",
    "                    \"ManagerB\": b,\n",
    "                    \"OverlapCount\": len(inter),\n",
    "                    \"OverlapValue_AsFiled_A\": a_overlap_val,\n",
    "                    \"OverlapValue_AsFiled_B\": b_overlap_val,\n",
    "                    \"OverlapWeightShare_A_Pct\": (a_overlap_val / a_total * 100) if a_total else 0.0,\n",
    "                    \"OverlapWeightShare_B_Pct\": (b_overlap_val / b_total * 100) if b_total else 0.0,\n",
    "                })\n",
    "\n",
    "    pairwise_weighted_overlap = pd.DataFrame(rows)\n",
    "    if not pairwise_weighted_overlap.empty:\n",
    "        pairwise_weighted_overlap = pairwise_weighted_overlap.sort_values(\n",
    "            [\"PERIODOFREPORT_TS\", \"OverlapWeightShare_A_Pct\", \"OverlapWeightShare_B_Pct\"],\n",
    "            ascending=[False, False, False]\n",
    "        )\n",
    "\n",
    "    # Fund summary: overlap exposure + concentration (per period)\n",
    "    overlap_flag = mgr_cusip_vals.merge(\n",
    "        overlap_cusips[[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"ManagersHolding\"]],\n",
    "        on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    overlap_flag[\"IsOverlapped\"] = overlap_flag[\"ManagersHolding\"].fillna(0) >= OVERLAP_MIN_MANAGERS\n",
    "\n",
    "    fund_summary = (\n",
    "        overlap_flag.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\"], dropna=False)\n",
    "        .agg(\n",
    "            NumPositions=(\"CUSIP\", \"nunique\"),\n",
    "            NumOverlappedPositions=(\"IsOverlapped\", \"sum\"),\n",
    "            OverlapWeightPct_Total=(\"WeightPct\", lambda s: s[overlap_flag.loc[s.index, \"IsOverlapped\"]].sum()),\n",
    "            LargestPositionWeightPct=(\"WeightPct\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    top10_conc = (\n",
    "        overlap_flag.sort_values([\"PERIODOFREPORT_TS\", \"manager\", \"WeightPct\"], ascending=[False, True, False])\n",
    "        .groupby([\"PERIODOFREPORT_TS\", \"manager\"], dropna=False)\n",
    "        .head(10)\n",
    "        .groupby([\"PERIODOFREPORT_TS\", \"manager\"], dropna=False)[\"WeightPct\"]\n",
    "        .sum()\n",
    "        .rename(\"Top10ConcentrationPct\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    fund_summary = fund_summary.merge(top10_conc, on=[\"PERIODOFREPORT_TS\", \"manager\"], how=\"left\")\n",
    "    fund_summary = fund_summary.sort_values([\"PERIODOFREPORT_TS\", \"OverlapWeightPct_Total\"], ascending=[False, False])\n",
    "\n",
    "    # Crowding summary\n",
    "    crowded_holdings = (\n",
    "        overlap_weights_long.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\", \"ManagersHolding\"], dropna=False)\n",
    "        .agg(\n",
    "            TotalValue_AsFiled_AllManagers=(\"PositionValue_AsFiled\", \"sum\"),\n",
    "            AvgWeightPct=(\"WeightPct\", \"mean\"),\n",
    "            MedianWeightPct=(\"WeightPct\", \"median\"),\n",
    "            MinWeightPct=(\"WeightPct\", \"min\"),\n",
    "            MaxWeightPct=(\"WeightPct\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"PERIODOFREPORT_TS\", \"ManagersHolding\", \"AvgWeightPct\", \"TotalValue_AsFiled_AllManagers\"],\n",
    "                     ascending=[False, False, False, False])\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # NEW: FUND CHANGES (latest vs previous per manager)\n",
    "    # ============================================================\n",
    "    fund_changes_long, fund_changes_summary = build_fund_changes(mgr_cusip_vals)\n",
    "\n",
    "    # Also write a “periods_selected” tab for transparency\n",
    "    periods_selected = picked.sort_values([\"CIK\", \"PERIODOFREPORT_TS\"], ascending=[True, False]).copy()\n",
    "    periods_selected[\"CIK\"] = periods_selected[\"CIK\"].astype(str).str.zfill(10)\n",
    "\n",
    "    # =========================\n",
    "    # WRITE OUTPUT\n",
    "    # =========================\n",
    "    out_file = safe_excel_writer_path(out_dir, \"13F_bulk_overlap_summary\")\n",
    "\n",
    "    with pd.ExcelWriter(out_file, engine=\"openpyxl\") as writer:\n",
    "        write_df_split(writer, periods_selected, \"periods_selected\")\n",
    "\n",
    "        write_df_split(writer, pairwise_overlap, \"pairwise_overlap\")\n",
    "        write_df_split(writer, pairwise_weighted_overlap, \"pairwise_weighted\")\n",
    "        write_df_split(writer, all_overlaps, \"all_overlaps\")\n",
    "        write_df_split(writer, overlap_weights_long, \"overlap_weights_long\")\n",
    "        write_df_split(writer, overlap_weights_matrix, \"overlap_weights_matrix\")\n",
    "        write_df_split(writer, common_all, \"common_all\")\n",
    "        write_df_split(writer, crowded_holdings, \"crowded_holdings\")\n",
    "        write_df_split(\n",
    "            writer,\n",
    "            fund_totals.sort_values([\"PERIODOFREPORT_TS\", \"Total13FValue_AsFiled\"], ascending=[False, False]),\n",
    "            \"fund_totals\"\n",
    "        )\n",
    "\n",
    "        # 1–4 idea-gen\n",
    "        write_df_split(writer, idea_ranker_overlaps, \"idea_ranker\")\n",
    "        write_df_split(writer, idea_quadrants, \"idea_quadrants\")\n",
    "        write_df_split(writer, fund_summary, \"fund_summary\")\n",
    "\n",
    "        # NEW change tabs\n",
    "        write_df_split(writer, fund_changes_long, \"fund_changes_long\")\n",
    "        write_df_split(writer, fund_changes_summary, \"fund_changes_summary\")\n",
    "\n",
    "        # Per-manager drilldown (small universes only)\n",
    "        for mgr, dfm in mgr_cusip_vals.groupby(\"manager\"):\n",
    "            sheet = clean_sheet_name(mgr)\n",
    "            dfm.sort_values([\"PERIODOFREPORT_TS\", \"WeightPct\"], ascending=[False, False]).to_excel(\n",
    "                writer, sheet_name=sheet, index=False\n",
    "            )\n",
    "\n",
    "    print(f\"\\nSaved: {out_file}\")\n",
    "    print(\"\\nUse these tabs for change detection:\")\n",
    "    print(\" - fund_changes_long: NEW / EXIT / INCREASE / DECREASE / UNCHANGED per CUSIP per manager.\")\n",
    "    print(\" - fund_changes_summary: counts + aggregate deltas per manager (latest vs previous).\")\n",
    "    print(\" - periods_selected: exactly which two report periods were pulled per CIK.\")\n",
    "    print(\"\\nIf the file name has a timestamp: you had the old workbook open in Excel.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_bulk_13f()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
