{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6289a83a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provide at least 2 CIKs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 635\u001b[39m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m - new_positions_consensus shows positions that are NEW in the latest period for >= \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    631\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNEW_CONSENSUS_MIN_MANAGERS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m managers.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[43minteractive_bulk_13f\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 471\u001b[39m, in \u001b[36minteractive_bulk_13f\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    469\u001b[39m target_ciks = [normalize_cik(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m cik_input.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x.strip()]\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target_ciks) < \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mProvide at least 2 CIKs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    473\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDiscovering SEC 13F dataset links...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    474\u001b[39m links = list_dataset_links(sec, max_items=\u001b[32m10\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Provide at least 2 CIKs."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "USER_AGENT = \"Name (your.email@domain.com) - 13F overlap research\"  # must include '@'\n",
    "MIN_SECONDS_BETWEEN_REQUESTS = 0.22\n",
    "\n",
    "BASE_OUTPUT_DIR = Path(r\"C:\\Users\\reigh\\Desktop\\Fin Statements\")\n",
    "OUT_FOLDER_NAME = \"13F_Bulk_Overlap\"\n",
    "\n",
    "# How many SEC bulk dataset ZIPs to auto-download (2 = last 2 filing cycles)\n",
    "N_LATEST_DATASETS_TO_FETCH = 2\n",
    "\n",
    "# Overlap behavior\n",
    "DISTINGUISH_OPTIONS = False          # if True: overlap key = CUSIP|PUTCALL (usually not recommended)\n",
    "EXCLUDE_OPTIONS_FROM_OVERLAP = True  # ignore rows with PUTCALL for overlap + weights/totals\n",
    "\n",
    "# Overlap definition (per period)\n",
    "OVERLAP_MIN_MANAGERS = 2\n",
    "\n",
    "# \"Consensus new positions\" definition\n",
    "NEW_CONSENSUS_MIN_MANAGERS = 2  # e.g., NEW in >=2 managers in the latest period\n",
    "\n",
    "# Change classification threshold (avoid tiny noise)\n",
    "WEIGHT_CHANGE_PCT_THRESHOLD = 0.05  # 0.05% weight move treated as \"unchanged\" band\n",
    "\n",
    "SEC_13F_DATASETS_PAGE = \"https://www.sec.gov/data-research/sec-markets-data/form-13f-data-sets\"\n",
    "\n",
    "EXCEL_MAX_ROWS = 1_000_000\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SEC CLIENT\n",
    "# =========================\n",
    "class SECClient:\n",
    "    def __init__(self, user_agent: str, min_interval_s: float = 0.22):\n",
    "        if not user_agent or \"@\" not in user_agent:\n",
    "            raise ValueError(\"Set USER_AGENT like 'Name (your@email.com) - purpose'.\")\n",
    "        self.s = requests.Session()\n",
    "        self.s.headers.update({\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        })\n",
    "        self.min_interval_s = min_interval_s\n",
    "        self._last_request_ts = 0.0\n",
    "\n",
    "    def _sleep_if_needed(self):\n",
    "        now = time.time()\n",
    "        dt = now - self._last_request_ts\n",
    "        if dt < self.min_interval_s:\n",
    "            time.sleep(self.min_interval_s - dt)\n",
    "\n",
    "    def get(self, url: str, stream: bool = False, max_retries: int = 6) -> requests.Response:\n",
    "        for attempt in range(max_retries):\n",
    "            self._sleep_if_needed()\n",
    "            r = self.s.get(url, timeout=60, stream=stream)\n",
    "            self._last_request_ts = time.time()\n",
    "            if r.status_code == 200:\n",
    "                return r\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                time.sleep(min(2 ** attempt, 16))\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "        raise RuntimeError(f\"Failed GET: {url}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_cik(x: str) -> str:\n",
    "    s = re.sub(r\"\\D\", \"\", x.strip())\n",
    "    if not s:\n",
    "        raise ValueError(f\"Bad CIK: {x}\")\n",
    "    return str(int(s)).zfill(10)\n",
    "\n",
    "def parse_dd_mon_yyyy(s: str) -> pd.Timestamp:\n",
    "    return pd.to_datetime(s, format=\"%d-%b-%Y\", errors=\"coerce\")\n",
    "\n",
    "def holding_key(df: pd.DataFrame) -> pd.Series:\n",
    "    if DISTINGUISH_OPTIONS:\n",
    "        return df[\"CUSIP\"].astype(str) + \"|\" + df[\"PUTCALL\"].fillna(\"\").astype(str)\n",
    "    return df[\"CUSIP\"].astype(str)\n",
    "\n",
    "def clean_sheet_name(s: str) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9 _-]\", \"\", str(s)).strip()\n",
    "    return (s[:31] or \"sheet\")\n",
    "\n",
    "def write_df_split(writer: pd.ExcelWriter, df: pd.DataFrame, sheet_base: str, max_rows: int = EXCEL_MAX_ROWS) -> None:\n",
    "    sheet_base = clean_sheet_name(sheet_base)\n",
    "    if df is None or df.empty:\n",
    "        pd.DataFrame().to_excel(writer, sheet_name=sheet_base[:31], index=False)\n",
    "        return\n",
    "\n",
    "    n = len(df)\n",
    "    if n <= max_rows:\n",
    "        df.to_excel(writer, sheet_name=sheet_base[:31], index=False)\n",
    "        return\n",
    "\n",
    "    k = 0\n",
    "    start = 0\n",
    "    while start < n:\n",
    "        k += 1\n",
    "        chunk = df.iloc[start:start + max_rows].copy()\n",
    "        suffix = f\"_{k}\"\n",
    "        name = (sheet_base[: (31 - len(suffix))] + suffix)[:31]\n",
    "        chunk.to_excel(writer, sheet_name=name, index=False)\n",
    "        start += max_rows\n",
    "\n",
    "def safe_excel_writer_path(out_dir: Path, base_name: str) -> Path:\n",
    "    \"\"\"\n",
    "    If the Excel file is open, write to a timestamped filename instead of crashing.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    base_path = out_dir / f\"{base_name}.xlsx\"\n",
    "    try:\n",
    "        with open(base_path, \"ab\"):\n",
    "            pass\n",
    "        return base_path\n",
    "    except PermissionError:\n",
    "        ts = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return out_dir / f\"{base_name}_{ts}.xlsx\"\n",
    "\n",
    "def find_file_by_token(folder: Path, token: str) -> Path:\n",
    "    token_u = token.upper()\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and token_u in p.name.upper() and p.suffix.lower() in {\".tsv\", \".txt\"}:\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Could not find a TSV/TXT file containing '{token}' under {folder}\")\n",
    "\n",
    "def compute_pairwise(keys_by_mgr: Dict[str, set]) -> pd.DataFrame:\n",
    "    mgrs = sorted(keys_by_mgr.keys())\n",
    "    rows = []\n",
    "    for i in range(len(mgrs)):\n",
    "        for j in range(i + 1, len(mgrs)):\n",
    "            a, b = mgrs[i], mgrs[j]\n",
    "            sa, sb = keys_by_mgr[a], keys_by_mgr[b]\n",
    "            inter = len(sa & sb)\n",
    "            union = len(sa | sb)\n",
    "            rows.append({\n",
    "                \"ManagerA\": a,\n",
    "                \"ManagerB\": b,\n",
    "                \"OverlapCount\": inter,\n",
    "                \"UnionCount\": union,\n",
    "                \"Jaccard\": (inter / union) if union else 0.0\n",
    "            })\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"ManagerA\", \"ManagerB\", \"OverlapCount\", \"UnionCount\", \"Jaccard\"])\n",
    "    return pd.DataFrame(rows).sort_values([\"OverlapCount\", \"Jaccard\"], ascending=False)\n",
    "\n",
    "def manager_display_name(merged: pd.DataFrame) -> pd.Series:\n",
    "    def f(r):\n",
    "        nm = r.get(\"FILINGMANAGER_NAME\", None)\n",
    "        if isinstance(nm, str) and nm.strip():\n",
    "            return nm.strip()\n",
    "        return f\"CIK{str(r['CIK']).zfill(10)}\"\n",
    "    return merged.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DATASET DISCOVERY + DOWNLOAD\n",
    "# =========================\n",
    "def list_dataset_links(sec: SECClient, max_items: int = 12) -> List[Tuple[str, str]]:\n",
    "    html = sec.get(SEC_13F_DATASETS_PAGE).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\") or \"\"\n",
    "        text = a.get_text(\" \", strip=True)\n",
    "        if href.endswith(\"_form13f.zip\"):\n",
    "            if href.startswith(\"/\"):\n",
    "                href = \"https://www.sec.gov\" + href\n",
    "            links.append((text, href))\n",
    "    # page is typically newest-to-oldest\n",
    "    return links[:max_items]\n",
    "\n",
    "def download_and_extract(sec: SECClient, zip_url: str, extract_dir: Path, zip_path: Path) -> None:\n",
    "    extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download\n",
    "    r = sec.get(zip_url, stream=True)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    # Extract\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD + FILTER TABLES\n",
    "# =========================\n",
    "def load_submission(submission_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(submission_path, sep=\"\\t\", dtype=str)\n",
    "    df[\"FILING_DATE_TS\"] = df[\"FILING_DATE\"].apply(parse_dd_mon_yyyy)\n",
    "    df[\"PERIODOFREPORT_TS\"] = df[\"PERIODOFREPORT\"].apply(parse_dd_mon_yyyy)\n",
    "    df[\"CIK\"] = df[\"CIK\"].astype(str).str.zfill(10)\n",
    "    return df\n",
    "\n",
    "def pick_latest_accessions_per_period(sub: pd.DataFrame, target_ciks: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One row per (CIK, PERIODOFREPORT) selecting latest FILING_DATE (so amendment wins).\n",
    "    \"\"\"\n",
    "    sub = sub[sub[\"SUBMISSIONTYPE\"].isin([\"13F-HR\", \"13F-HR/A\"])].copy()\n",
    "    sub = sub[sub[\"CIK\"].isin(target_ciks)].copy()\n",
    "    sub = sub.sort_values([\"CIK\", \"PERIODOFREPORT_TS\", \"FILING_DATE_TS\"], ascending=[True, True, False])\n",
    "    picked = sub.drop_duplicates(subset=[\"CIK\", \"PERIODOFREPORT_TS\"], keep=\"first\")\n",
    "    return picked[[\n",
    "        \"ACCESSION_NUMBER\", \"CIK\", \"SUBMISSIONTYPE\", \"FILING_DATE\", \"PERIODOFREPORT\",\n",
    "        \"FILING_DATE_TS\", \"PERIODOFREPORT_TS\"\n",
    "    ]].copy()\n",
    "\n",
    "def load_coverpage(coverpage_path: Path, accessions: set) -> pd.DataFrame:\n",
    "    df = pd.read_csv(coverpage_path, sep=\"\\t\", dtype=str)\n",
    "    df = df[df[\"ACCESSION_NUMBER\"].isin(accessions)].copy()\n",
    "    return df\n",
    "\n",
    "def load_infotable_chunked(infotable_path: Path, accessions: set) -> pd.DataFrame:\n",
    "    usecols = [\n",
    "        \"ACCESSION_NUMBER\", \"INFOTABLE_SK\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "        \"CUSIP\", \"FIGI\", \"VALUE\", \"SSHPRNAMT\", \"SSHPRNAMTTYPE\", \"PUTCALL\",\n",
    "        \"INVESTMENTDISCRETION\", \"OTHERMANAGER\", \"VOTING_AUTH_SOLE\", \"VOTING_AUTH_SHARED\", \"VOTING_AUTH_NONE\"\n",
    "    ]\n",
    "    chunks = []\n",
    "    for ch in pd.read_csv(infotable_path, sep=\"\\t\", dtype=str, usecols=lambda c: c in usecols, chunksize=400_000):\n",
    "        ch = ch[ch[\"ACCESSION_NUMBER\"].isin(accessions)]\n",
    "        if not ch.empty:\n",
    "            chunks.append(ch)\n",
    "    if not chunks:\n",
    "        return pd.DataFrame(columns=usecols)\n",
    "\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df[\"CUSIP\"] = df[\"CUSIP\"].astype(str).str.replace(r\"\\s+\", \"\", regex=True)\n",
    "\n",
    "    for c in [\"VALUE\", \"SSHPRNAMT\", \"VOTING_AUTH_SOLE\", \"VOTING_AUTH_SHARED\", \"VOTING_AUTH_NONE\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "\n",
    "    if \"PUTCALL\" in df.columns:\n",
    "        df[\"PUTCALL\"] = df[\"PUTCALL\"].replace(\"\", pd.NA)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_one_dataset(extract_dir: Path, target_ciks: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      merged_rows: infotable rows joined with submission (+ coverpage if present)\n",
    "      picked: picked submission rows (accessions selected) for transparency\n",
    "    \"\"\"\n",
    "    submission_path = find_file_by_token(extract_dir, \"SUBMISSION\")\n",
    "    infotable_path = find_file_by_token(extract_dir, \"INFOTABLE\")\n",
    "    coverpage_path = None\n",
    "    try:\n",
    "        coverpage_path = find_file_by_token(extract_dir, \"COVERPAGE\")\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    sub = load_submission(submission_path)\n",
    "    picked_pp = pick_latest_accessions_per_period(sub, target_ciks)\n",
    "    if picked_pp.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    accessions = set(picked_pp[\"ACCESSION_NUMBER\"].tolist())\n",
    "\n",
    "    cover = pd.DataFrame()\n",
    "    if coverpage_path is not None:\n",
    "        cover = load_coverpage(coverpage_path, accessions)\n",
    "\n",
    "    info = load_infotable_chunked(infotable_path, accessions)\n",
    "    if info.empty:\n",
    "        return pd.DataFrame(), picked_pp\n",
    "\n",
    "    merged = info.merge(\n",
    "        picked_pp[[\"ACCESSION_NUMBER\", \"CIK\", \"SUBMISSIONTYPE\", \"FILING_DATE\", \"PERIODOFREPORT\", \"FILING_DATE_TS\", \"PERIODOFREPORT_TS\"]],\n",
    "        on=\"ACCESSION_NUMBER\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    if not cover.empty and \"FILINGMANAGER_NAME\" in cover.columns:\n",
    "        merged = merged.merge(\n",
    "            cover[[\"ACCESSION_NUMBER\", \"FILINGMANAGER_NAME\", \"ISAMENDMENT\", \"AMENDMENTTYPE\"]],\n",
    "            on=\"ACCESSION_NUMBER\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    else:\n",
    "        merged[\"FILINGMANAGER_NAME\"] = None\n",
    "        merged[\"ISAMENDMENT\"] = None\n",
    "        merged[\"AMENDMENTTYPE\"] = None\n",
    "\n",
    "    merged[\"manager\"] = manager_display_name(merged)\n",
    "\n",
    "    return merged, picked_pp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CHANGE ANALYSIS\n",
    "# =========================\n",
    "def build_fund_changes(mgr_cusip_vals: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compare latest period vs previous period for each manager based on PERIODOFREPORT_TS.\n",
    "    \"\"\"\n",
    "    if mgr_cusip_vals.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    periods_by_mgr = (\n",
    "        mgr_cusip_vals[[\"manager\", \"PERIODOFREPORT_TS\"]].drop_duplicates()\n",
    "        .sort_values([\"manager\", \"PERIODOFREPORT_TS\"], ascending=[True, False])\n",
    "    )\n",
    "    mgr_to_two = (\n",
    "        periods_by_mgr.groupby(\"manager\", dropna=False)[\"PERIODOFREPORT_TS\"]\n",
    "        .apply(lambda s: list(s.head(2)))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for mgr, per_list in mgr_to_two.items():\n",
    "        if len(per_list) < 2:\n",
    "            continue\n",
    "        p_curr_ts, p_prev_ts = per_list[0], per_list[1]\n",
    "\n",
    "        curr = mgr_cusip_vals[(mgr_cusip_vals[\"manager\"] == mgr) & (mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == p_curr_ts)].copy()\n",
    "        prev = mgr_cusip_vals[(mgr_cusip_vals[\"manager\"] == mgr) & (mgr_cusip_vals[\"PERIODOFREPORT_TS\"] == p_prev_ts)].copy()\n",
    "\n",
    "        j = curr.merge(prev, on=[\"manager\", \"CUSIP\"], how=\"outer\", suffixes=(\"_curr\", \"_prev\"))\n",
    "\n",
    "        j[\"PERIOD_CURR_TS\"] = p_curr_ts\n",
    "        j[\"PERIOD_PREV_TS\"] = p_prev_ts\n",
    "\n",
    "        for col in [\"PERIODOFREPORT\", \"NAMEOFISSUER\", \"TITLEOFCLASS\"]:\n",
    "            j[col] = j.get(f\"{col}_curr\").combine_first(j.get(f\"{col}_prev\"))\n",
    "\n",
    "        for col in [\"PositionValue_AsFiled\", \"Total13FValue_AsFiled\", \"WeightPct\"]:\n",
    "            j[f\"{col}_curr\"] = j[f\"{col}_curr\"].fillna(0)\n",
    "            j[f\"{col}_prev\"] = j[f\"{col}_prev\"].fillna(0)\n",
    "\n",
    "        j[\"DeltaValue_AsFiled\"] = j[\"PositionValue_AsFiled_curr\"] - j[\"PositionValue_AsFiled_prev\"]\n",
    "        j[\"DeltaWeightPct\"] = j[\"WeightPct_curr\"] - j[\"WeightPct_prev\"]\n",
    "\n",
    "        def classify(r):\n",
    "            had_prev = r[\"PositionValue_AsFiled_prev\"] > 0\n",
    "            has_curr = r[\"PositionValue_AsFiled_curr\"] > 0\n",
    "            if (not had_prev) and has_curr:\n",
    "                return \"NEW\"\n",
    "            if had_prev and (not has_curr):\n",
    "                return \"EXIT\"\n",
    "            if abs(r[\"DeltaWeightPct\"]) < WEIGHT_CHANGE_PCT_THRESHOLD:\n",
    "                return \"UNCHANGED\"\n",
    "            return \"INCREASE\" if r[\"DeltaWeightPct\"] > 0 else \"DECREASE\"\n",
    "\n",
    "        j[\"ChangeType\"] = j.apply(classify, axis=1)\n",
    "\n",
    "        out_cols = [\n",
    "            \"manager\",\n",
    "            \"PERIODOFREPORT\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\",\n",
    "            \"PERIOD_PREV_TS\", \"PERIOD_CURR_TS\",\n",
    "            \"PositionValue_AsFiled_prev\", \"WeightPct_prev\",\n",
    "            \"PositionValue_AsFiled_curr\", \"WeightPct_curr\",\n",
    "            \"DeltaValue_AsFiled\", \"DeltaWeightPct\",\n",
    "            \"ChangeType\"\n",
    "        ]\n",
    "        rows.append(j[out_cols])\n",
    "\n",
    "    fund_changes_long = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "    if fund_changes_long.empty:\n",
    "        return fund_changes_long, pd.DataFrame()\n",
    "\n",
    "    fund_changes_long[\"PERIOD_PREV\"] = fund_changes_long[\"PERIOD_PREV_TS\"].dt.strftime(\"%d-%b-%Y\")\n",
    "    fund_changes_long[\"PERIOD_CURR\"] = fund_changes_long[\"PERIOD_CURR_TS\"].dt.strftime(\"%d-%b-%Y\")\n",
    "\n",
    "    summ = (\n",
    "        fund_changes_long.groupby([\"manager\", \"PERIOD_PREV\", \"PERIOD_CURR\"], dropna=False)\n",
    "        .agg(\n",
    "            NewPositions=(\"ChangeType\", lambda s: (s == \"NEW\").sum()),\n",
    "            Exits=(\"ChangeType\", lambda s: (s == \"EXIT\").sum()),\n",
    "            Increases=(\"ChangeType\", lambda s: (s == \"INCREASE\").sum()),\n",
    "            Decreases=(\"ChangeType\", lambda s: (s == \"DECREASE\").sum()),\n",
    "            Unchanged=(\"ChangeType\", lambda s: (s == \"UNCHANGED\").sum()),\n",
    "            SumAbsDeltaWeightPct=(\"DeltaWeightPct\", lambda x: x.abs().sum()),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"SumAbsDeltaWeightPct\"], ascending=[False])\n",
    "    )\n",
    "\n",
    "    fund_changes_long = fund_changes_long.sort_values(\n",
    "        [\"manager\", \"ChangeType\", \"DeltaWeightPct\"],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "\n",
    "    return fund_changes_long, summ\n",
    "\n",
    "\n",
    "def build_new_positions_consensus(fund_changes_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Consensus NEW positions in the latest period: positions that are NEW for >= NEW_CONSENSUS_MIN_MANAGERS managers.\n",
    "    \"\"\"\n",
    "    if fund_changes_long.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    new_rows = fund_changes_long[fund_changes_long[\"ChangeType\"] == \"NEW\"].copy()\n",
    "    if new_rows.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Identify latest \"current period\" across the dataset\n",
    "    latest_curr_ts = new_rows[\"PERIOD_CURR_TS\"].max()\n",
    "    new_rows = new_rows[new_rows[\"PERIOD_CURR_TS\"] == latest_curr_ts].copy()\n",
    "\n",
    "    if new_rows.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    g = new_rows.groupby([\"PERIOD_CURR_TS\", \"PERIOD_CURR\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\"], dropna=False)\n",
    "\n",
    "    consensus = g.agg(\n",
    "        ManagersNew=(\"manager\", \"nunique\"),\n",
    "        TotalNewWeightPct=(\"WeightPct_curr\", \"sum\"),\n",
    "        AvgNewWeightPct=(\"WeightPct_curr\", \"mean\"),\n",
    "        MedianNewWeightPct=(\"WeightPct_curr\", \"median\"),\n",
    "        MaxNewWeightPct=(\"WeightPct_curr\", \"max\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Determine top holder among NEW adopters\n",
    "    top_holder = (\n",
    "        new_rows.sort_values([\"CUSIP\", \"WeightPct_curr\"], ascending=[True, False])\n",
    "        .groupby([\"CUSIP\"], dropna=False)\n",
    "        .head(1)[[\"CUSIP\", \"manager\", \"WeightPct_curr\"]]\n",
    "        .rename(columns={\"manager\": \"TopNewHolder\", \"WeightPct_curr\": \"TopNewHolderWeightPct\"})\n",
    "    )\n",
    "    consensus = consensus.merge(top_holder, on=\"CUSIP\", how=\"left\")\n",
    "\n",
    "    # Manager list (small: 5–10 CIKs)\n",
    "    mgr_list = (\n",
    "        new_rows.groupby(\"CUSIP\")[\"manager\"]\n",
    "        .apply(lambda s: \", \".join(sorted(set(map(str, s)))))\n",
    "        .rename(\"ManagersNewList\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    consensus = consensus.merge(mgr_list, on=\"CUSIP\", how=\"left\")\n",
    "\n",
    "    # Filter to “consensus” threshold\n",
    "    consensus = consensus[consensus[\"ManagersNew\"] >= NEW_CONSENSUS_MIN_MANAGERS].copy()\n",
    "\n",
    "    # Sort: strongest consensus first\n",
    "    consensus = consensus.sort_values(\n",
    "        [\"ManagersNew\", \"TotalNewWeightPct\", \"MaxNewWeightPct\"],\n",
    "        ascending=[False, False, False]\n",
    "    )\n",
    "\n",
    "    return consensus\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def interactive_bulk_13f():\n",
    "    sec = SECClient(USER_AGENT, MIN_SECONDS_BETWEEN_REQUESTS)\n",
    "\n",
    "    cik_input = input(\"Enter manager CIKs (comma-separated): \").strip()\n",
    "    target_ciks = [normalize_cik(x) for x in cik_input.split(\",\") if x.strip()]\n",
    "    if len(target_ciks) < 2:\n",
    "        raise ValueError(\"Provide at least 2 CIKs.\")\n",
    "\n",
    "    print(\"\\nDiscovering SEC 13F dataset links...\")\n",
    "    links = list_dataset_links(sec, max_items=10)\n",
    "    if not links:\n",
    "        raise RuntimeError(\"Could not find dataset links on the SEC page.\")\n",
    "\n",
    "    # Auto-select latest N datasets (no prompt)\n",
    "    selected = links[:N_LATEST_DATASETS_TO_FETCH]\n",
    "    print(\"\\nAuto-selected datasets:\")\n",
    "    for i, (label, url) in enumerate(selected, start=1):\n",
    "        print(f\"  {i}) {label} | {url}\")\n",
    "\n",
    "    out_dir = BASE_OUTPUT_DIR / OUT_FOLDER_NAME\n",
    "    datasets_dir = out_dir / \"datasets\"\n",
    "    datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    merged_frames = []\n",
    "    picked_frames = []\n",
    "\n",
    "    for i, (label, url) in enumerate(selected, start=1):\n",
    "        extract_dir = datasets_dir / f\"extracted_{i:02d}\"\n",
    "        zip_path = datasets_dir / f\"dataset_{i:02d}.zip\"\n",
    "\n",
    "        print(f\"\\nDownloading + extracting dataset {i}/{len(selected)}...\")\n",
    "        download_and_extract(sec, url, extract_dir, zip_path)\n",
    "\n",
    "        merged_i, picked_i = process_one_dataset(extract_dir, target_ciks)\n",
    "        if not picked_i.empty:\n",
    "            picked_i = picked_i.copy()\n",
    "            picked_i[\"DATASET_IDX\"] = i\n",
    "            picked_i[\"DATASET_LABEL\"] = label\n",
    "            picked_frames.append(picked_i)\n",
    "\n",
    "        if not merged_i.empty:\n",
    "            merged_i = merged_i.copy()\n",
    "            merged_i[\"DATASET_IDX\"] = i\n",
    "            merged_i[\"DATASET_LABEL\"] = label\n",
    "            merged_frames.append(merged_i)\n",
    "\n",
    "    if not merged_frames:\n",
    "        raise RuntimeError(\"No holdings rows were found for your CIKs across the selected datasets.\")\n",
    "\n",
    "    merged = pd.concat(merged_frames, ignore_index=True)\n",
    "    picked_all = pd.concat(picked_frames, ignore_index=True) if picked_frames else pd.DataFrame()\n",
    "\n",
    "    # Base universe (option filter)\n",
    "    base = merged.copy()\n",
    "    if EXCLUDE_OPTIONS_FROM_OVERLAP and \"PUTCALL\" in base.columns:\n",
    "        base = base[base[\"PUTCALL\"].isna()]\n",
    "\n",
    "    # Fund totals + weights (as filed)\n",
    "    mgr_cusip_vals = (\n",
    "        base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\", \"CUSIP\"], dropna=False)[\"VALUE\"]\n",
    "        .sum(min_count=1)\n",
    "        .rename(\"PositionValue_AsFiled\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    fund_totals = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\"], dropna=False)[\"PositionValue_AsFiled\"]\n",
    "        .sum(min_count=1)\n",
    "        .rename(\"Total13FValue_AsFiled\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    mgr_cusip_vals = mgr_cusip_vals.merge(\n",
    "        fund_totals, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"manager\"], how=\"left\"\n",
    "    )\n",
    "    mgr_cusip_vals[\"WeightPct\"] = (mgr_cusip_vals[\"PositionValue_AsFiled\"] / mgr_cusip_vals[\"Total13FValue_AsFiled\"]) * 100\n",
    "\n",
    "    labels = (\n",
    "        base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[[\"NAMEOFISSUER\", \"TITLEOFCLASS\"]]\n",
    "        .first()\n",
    "        .reset_index()\n",
    "    )\n",
    "    mgr_cusip_vals = mgr_cusip_vals.merge(labels, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "\n",
    "    # Overlap counts per CUSIP per period\n",
    "    cusip_mgr_counts = (\n",
    "        mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"manager\"]\n",
    "        .nunique()\n",
    "        .rename(\"ManagersHolding\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    overlap_cusips = cusip_mgr_counts[cusip_mgr_counts[\"ManagersHolding\"] >= OVERLAP_MIN_MANAGERS].copy()\n",
    "\n",
    "    all_overlaps = (\n",
    "        overlap_cusips.merge(\n",
    "            mgr_cusip_vals.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], dropna=False)[\"PositionValue_AsFiled\"]\n",
    "            .sum(min_count=1).rename(\"TotalValue_AsFiled_AllManagers\").reset_index(),\n",
    "            on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .merge(labels, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"left\")\n",
    "        .sort_values([\"PERIODOFREPORT_TS\", \"ManagersHolding\", \"TotalValue_AsFiled_AllManagers\"],\n",
    "                     ascending=[False, False, False])\n",
    "    )\n",
    "\n",
    "    overlap_weights_long = (\n",
    "        mgr_cusip_vals.merge(overlap_cusips, on=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\"], how=\"inner\")\n",
    "        .sort_values([\"PERIODOFREPORT_TS\", \"ManagersHolding\", \"CUSIP\", \"WeightPct\"],\n",
    "                     ascending=[False, False, True, False])\n",
    "    )\n",
    "\n",
    "    overlap_weights_matrix = overlap_weights_long.pivot_table(\n",
    "        index=[\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\", \"CUSIP\", \"NAMEOFISSUER\", \"TITLEOFCLASS\", \"ManagersHolding\"],\n",
    "        columns=\"manager\",\n",
    "        values=\"WeightPct\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # Pairwise overlap per period\n",
    "    pairwise_frames = []\n",
    "    for (period, period_ts), dfp in base.groupby([\"PERIODOFREPORT\", \"PERIODOFREPORT_TS\"], dropna=False):\n",
    "        keys_by_mgr = {}\n",
    "        for mgr, dfm in dfp.groupby(\"manager\"):\n",
    "            keys_by_mgr[mgr] = set(holding_key(dfm).dropna().tolist())\n",
    "        pw = compute_pairwise(keys_by_mgr)\n",
    "        if not pw.empty:\n",
    "            pw.insert(0, \"PERIODOFREPORT\", period)\n",
    "            pw.insert(1, \"PERIODOFREPORT_TS\", period_ts)\n",
    "            pairwise_frames.append(pw)\n",
    "    pairwise_overlap = pd.concat(pairwise_frames, ignore_index=True) if pairwise_frames else pd.DataFrame()\n",
    "\n",
    "    # Fund changes + consensus new\n",
    "    fund_changes_long, fund_changes_summary = build_fund_changes(mgr_cusip_vals)\n",
    "    new_positions_consensus = build_new_positions_consensus(fund_changes_long)\n",
    "\n",
    "    # =========================\n",
    "    # WRITE OUTPUT\n",
    "    # =========================\n",
    "    out_file = safe_excel_writer_path(out_dir, \"13F_bulk_overlap_summary\")\n",
    "\n",
    "    with pd.ExcelWriter(out_file, engine=\"openpyxl\") as writer:\n",
    "        \n",
    "\n",
    "        write_df_split(writer, pairwise_overlap, \"pairwise_overlap\")\n",
    "        write_df_split(writer, all_overlaps, \"all_overlaps\")\n",
    "        \n",
    "        write_df_split(writer, overlap_weights_matrix, \"overlap_weights_matrix\")\n",
    "        write_df_split(writer, overlap_weights_matrix, \"overlap_weights_long\")\n",
    "        \n",
    "\n",
    "        write_df_split(writer, fund_changes_long, \"fund_changes_long\")\n",
    "        write_df_split(writer, fund_changes_summary, \"fund_changes_summary\")\n",
    "        write_df_split(writer, new_positions_consensus, \"new_positions_consensus\")\n",
    "\n",
    "        # Optional: per-manager holdings (sorted by period and weight)\n",
    "        for mgr, dfm in mgr_cusip_vals.groupby(\"manager\"):\n",
    "            sheet = clean_sheet_name(mgr)\n",
    "            dfm.sort_values([\"PERIODOFREPORT_TS\", \"WeightPct\"], ascending=[False, False]).to_excel(\n",
    "                writer, sheet_name=sheet, index=False\n",
    "            )\n",
    "\n",
    "    print(f\"\\nSaved: {out_file}\")\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\" - No dataset prompt anymore: the script auto-downloads the latest 2 SEC bulk 13F datasets.\")\n",
    "    print(\" - If the output filename has a timestamp, your previous workbook was open in Excel.\")\n",
    "    print(\" - new_positions_consensus shows positions that are NEW in the latest period for >= \"\n",
    "          f\"{NEW_CONSENSUS_MIN_MANAGERS} managers.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_bulk_13f()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
